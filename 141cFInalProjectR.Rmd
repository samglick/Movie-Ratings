---
title: "141cFinalProject"
author: "Samuel J Glick, Alec S Lin, Vincent A Barletta"
date: "2024-02-25"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r Unzipping}
unzip("moviesZip.zip")
```
```{r Reading Tsv's}
library(data.table)
title <- fread("title.basics.tsv")
name <- fread("name.basics.tsv")
principals <- fread("title.principals.tsv")
ratings <- fread("title.ratings.tsv")
```
```{r}
dim(title)
dim(name)
dim(principals)
dim(ratings)
```
Pre Sorting Initial Data: 


# Data Descritions
Rating is about the ratings of each film I think there 
Titles is about the information on the title, such as genre and other things originally 10,000,000 Initially
Principals is about who worked on each title and is our longest data frame by far. 60,000,000 Initially
Name is about the actor and their information

#10:12-10:20 8 Minutes How long will it take to load like 100 million rows into r
8 minutes
```{r Unpacking the files without having to load them each time}
title<-readRDS("title.rds")
name<-readRDS("name.rds")
princ<-readRDS("principals.rds")
rate<-readRDS("ratings.rds")
```
Initials:
title: 10.5 mil by 9
name: 13.2 mil by 6
princ: 60.7 mil by 6
rate: 1.39 mil by 3
Whole Data Set: 750 Thousand

Post Sorting:

#Filtering Ideas
All cleaning items will limit the total number of titles we are appending to the name information data frame

Only titles with at least 100 votes
Only titles that are tvMovie or movie
Only non adult titles
Only titles that are at least sixty minutes long


Sorted Rating: went from 1.39 mil to 350 thousand 75 percent decrease
```{r Filtering the Rate Data Frame}
#Filtering Rate
head(ratings)
rate <- ratings[as.integer(ratings$numVotes) > 100]
hist(ratings$averageRating)

#Filtering Title
names(title)
unique(title$titleType)
title<-title[title$titleType=="movie"]
title<-title[title$isAdult==0]
title<-title[as.integer(title$startYear)>1940]
title<-title[as.integer(title$runtimeMinutes)>60]

#Creating Movie Data by Merging, can we optimize merging the data, probably should drop columns and 
rateTitle<-merge(title,rate, by = "tconst")
namePrinc<-merge(principals,name, by = "nconst")
movieData<-merge(namePrinc,rateTitle, by="tconst")
saveRDS(movieData,"movieData.rds")
```
Filtered title went from 10.5 million to 276782 thousand


Really long chunk. 


In our sorted data set we have 79 thousand titles, and it is a dimension of 765370 by 19 with each actor their role, in each movie that they played in. 

79 Thousand Titles
328 Thousand Different Names

Saving the large data set into our rds. format so we can quickly access it. 
```{r Reading in all movieData }
movieData<-readRDS("movieData.rds")
```


Histograms, Initial Plots, No Prediction Here.
```{r Exploratory Data Analysis}
unqTit<-length(unique(movieData$tconst))
unqTit
unqAct<-length(unique(movieData$nconst))
unqAct

library(dplyr)
actPerMov <- data %>%
  group_by(tconst) %>%
  summarise(UniqueActors = n_distinct(nconst))
summary(actPerMov$UniqueActors)
hist(actPerMov$UniqueActors)
plot(data$numVotes,data$averageRating)
numAvg<-lm(data$averageRating~data$numVotes)
summary(numAvg)
abline(numAvg)
```
Initial linear combination model, lets see how long it takes to run, we are predicting for 79 thousand different titles, and we have to predict each actor in that movie's average rating for all their prior work for that title. We do not want to use future leakage when predicting current event, so therefore, would using a time series make more sense? This large very time staking block of code is testing out the intial linear combination (Averages) model to predict all of the movies in our data set. This could be considered LOOCV for the entire data set with our null model being.

Basic Average
Moving Average
Exponential Smoothing (Type of Average)
Regression Models with Time Decay Factors
Actor's Career Trajectory Analysis (Time Series Method)
Mixed Effects Models (combining the actors abilities with the titles that they were in
Network Analysis)
# Clustering (A List, B List, Rising Stars, use that instead of each individual actor to predict the ratings)

The most basic historical data linear combination method

# New Variable (Predicted Cast Rating)
Creation of historical average as time increases, we see the average of each actors preformances up to the start year. That is historical average. Then we average that over the title for the new prdicted cast rating as a aggregate value of what we would expect from a preformance with that exact cast up to that exact point all assuming that each actor contributes the same amount to each movie equally. 

Here we create a new feature from the data that is the average preformance of each name up to that project. That is called historicalRating then we average that for each title and call that predictedRating, really it is not a prediction, only an average of prior data. It is just another feature we created in our data to specifically avoid future bleeding.
```{r}
library(data.table)
library(ggplot2)

# Ensure movieData is a data.table and correct data types
setDT(movieData)
movieData[, startYear := as.numeric(startYear)]

# Prepare a copy for self-joining
data_copy <- copy(movieData)

# Compute historical average ratings (self-join) - This step seems conceptualized incorrectly
# First, ensure historical data is correctly aggregated:
actor_hist_avg <- movieData[, .(avgRating = mean(averageRating, na.rm = TRUE)), by = .(nconst, startYear)]

# Now, perform operations intended for predicting ratings based on historical data
# Ensure only past performances are used for each actor's rating prediction
movieData[, historicalAvgRating := actor_hist_avg[movieData, on = .(nconst, startYear < startYear), mean(avgRating), by = .EACHI]$V1]
title_avg_ratings <- movieData[, .(predictedRating = mean(historicalAvgRating, na.rm = TRUE)), by = tconst]

# Join the calculated average ratings back into movieData
setkey(movieData, tconst)
setkey(title_avg_ratings, tconst)
movieData[, predictedRating := title_avg_ratings[.SD, on = .(tconst), x.predictedRating]]

# Join the calculated average ratings back into movieData

# Correcting the approach to sample and predict:
set.seed(123)  # Setting seed for reproducibility
sample_titles <- movieData[sample(.N, 200)]  # Sample titles

sampled<-data.frame(sample_titles$tconst,sample_titles$averageRating, sample_titles$predictedRating)

# Plotting
ggplot(sample_titles, aes(x = averageRating, y = predictedRating)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Movie Ratings (Sample)",
       x = "Actual Average Rating",
       y = "Predicted Average Rating") +
  theme_minimal()


ggplot(movieData, aes(x = averageRating, y = predictedRating)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Movie Ratings Whole Data Set",
       x = "Actual Average Rating",
       y = "Predicted Average Rating") +
  theme_minimal()

ggplot(movieData, aes(x = startYear, y = (predictedRating-averageRating)^2)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Historical Average Squared Residuals Over Time (Sample)",
       x = "Time",
       y = "Residual of Predicted Rating") +
  theme_minimal()


sample_titles <- movieData[sample(.N, 10000)]
numericalLM<-lm(averageRating ~ predictedRating + startYear + as.numeric(runtimeMinutes) + numVotes, data = sample_titles)
summary(numericalLM)
```
```{r Regressions}
saveRDS(movieData, "movieData.rds")
movieData$genre <- sapply(strsplit(movieData$genre, ","), `[`, 1)

sample_titles <- na.omit(movieData[sample(.N, 25000)])

predictedLin<-lm(averageRating~predictedRating, data = sample_titles)
summary(predictedLin)

predictedPoly <- lm(averageRating ~ poly(predictedRating, 3), data = sample_titles)
summary(predictedPoly)

bigModel<-lm(averageRating~poly(predictedRating, 3) + as.numeric(runtimeMinutes) + startYear + numVotes + genre * category + predictedRating*startYear + predictedRating*as.numeric(runtimeMinutes), data = sample_titles)
summary(bigModel)

genreAOV<-aov(averageRating~genre, data = sample_titles)
summary(genreAOV)
```

New classification method to make our data more suitable for regression. New Variables are prop rising, prop nobody, prop a, prop c, per title basis. Lets try an initial regression next using our new features. 

```{r Packages}
install.packages("foreach")
install.packages("doParellel")
```

# Name Classification (A List, B List, C List, Nobody, Rising Star)
Check out the proportion of a, b, and c listers as well as nobody's and also the rising stars in your data set. We can change the thresholds of each one to get different values, but these would be a nice regression friendly way of using our data. 
```{r  Turn The Data Into A, B, C, Nobodys, Rising Stars, Data Should }
library(data.table)
library(foreach)
library(doParallel)

create_actor_proportions_parallel <- function(movieData) {
    setDT(movieData)
    movieData[, startYear := as.numeric(startYear)]
    
    # Register parallel backend to use multiple cores
    cl <- makeCluster(detectCores() - 1)  # Leave one core free
    registerDoParallel(cl)
    
    # Prepare historical data: Aggregate by actor and year in parallel
    actorHistory <- foreach(n = unique(movieData$nconst), .combine = rbind, .packages = 'data.table') %dopar% {
        tempData <- movieData[nconst == n]
        tempData[, .(AverageRating = mean(averageRating), TotalMovies = .N, LatestYear = max(startYear)), by = .(nconst)]
    }
    
    # Define thresholds for categories
    thresholds <- list('A-List' = list('minRating' = 7, 'minMovies' = 10),
                       'B-List' = list('minRating' = 5, 'minMovies' = 5),
                       'Rising Star' = list('minRating' = 7, 'minMovies' = 3),
                       'C-List' = list('minRating' = 4, 'minMovies' = 2),
                       'Nobody' = list('minRating' = 0, 'minMovies' = 0))
    
    # Assign categories based on historical data
    actorHistory[, Category := 
                 fifelse(TotalMovies >= thresholds[['A-List']]$minMovies & AverageRating >= thresholds[['A-List']]$minRating, 'A-List',
                 fifelse(TotalMovies >= thresholds[['B-List']]$minMovies & AverageRating >= thresholds[['B-List']]$minRating, 'B-List',
                 fifelse(TotalMovies >= thresholds[['Rising Star']]$minMovies & AverageRating >= thresholds[['Rising Star']]$minRating & (LatestYear == max(movieData$startYear)), 'Rising Star',
                 fifelse(TotalMovies >= thresholds[['C-List']]$minMovies & AverageRating >= thresholds[['C-List']]$minRating, 'C-List',
                 'Nobody'))))]

    # Merge categories back to the original data
    movieData <- merge(movieData, actorHistory[, .(nconst, Category)], by = "nconst", all.x = TRUE)
    
    # Calculate proportions of each category in each movie
    proportions <- movieData[, .(Count = .N), by = .(tconst, Category)]
    proportions[, Proportion := Count / sum(Count), by = tconst]
    
    # Spread the proportions into separate columns for each category
    proportionColumns <- dcast(proportions, tconst ~ Category, value.var = "Proportion", fill = 0)
    
    # Merge the proportions back into the original movie data
    movieData <- merge(movieData, proportionColumns, by = "tconst", all.x = TRUE)
    
    # Stop the parallel cluster after completion
    stopCluster(cl)
    
    # Return the updated movie data with proportions
    return(movieData)
}

# Apply the parallel function to your dataset
updatedMovieData <- create_actor_proportions_parallel(movieData)
```


